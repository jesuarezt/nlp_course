{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "303d9d7f-d97e-4164-ac4a-31af770822a0",
   "metadata": {},
   "source": [
    "# Taller de NLP: Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 894,
   "id": "91991ad2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T08:21:58.509533Z",
     "start_time": "2022-02-14T08:21:58.495238Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The watermark extension is already loaded. To reload it, use:\n",
      "  %reload_ext watermark\n",
      "Author: Santiago Alferez\n",
      "\n",
      "numpy     : 1.21.2\n",
      "pandas    : 1.3.5\n",
      "re        : 2.2.1\n",
      "nltk      : 3.6.7\n",
      "sklearn   : 1.0.2\n",
      "matplotlib: 3.5.0\n",
      "scipy     : 1.7.3\n",
      "gensim    : 4.0.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#%load_ext watermark\n",
    "#%watermark -a 'Santiago Alferez' --iversions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4deb9569-4d68-40a4-9398-84a8f533f20a",
   "metadata": {},
   "source": [
    "Para este taller deberás disponer de algunas librerías como scikit-learn, NLTK, y GenSim. Se recomienda revisar la [documentación de GenSim](https://radimrehurek.com/gensim/auto_examples/index.html#documentation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb737da-3eeb-4cca-bc6e-ce1c55c82076",
   "metadata": {},
   "source": [
    "## Cargando un modelo  en GenSim y análisis\n",
    "A continuación cargaremos un modelo que no pesa tanto `glove-twitter-50`. Hay modelos más completos y con mayor número de dimensiones en este [link](https://github.com/RaRe-Technologies/gensim-data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7c407cf-1aef-4239-91fb-fc6bd01868ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T04:06:59.690028Z",
     "start_time": "2022-02-14T04:06:59.402957Z"
    },
    "execution": {
     "iopub.execute_input": "2022-02-14T03:27:46.900492Z",
     "iopub.status.busy": "2022-02-14T03:27:46.900171Z",
     "iopub.status.idle": "2022-02-14T03:27:46.914919Z",
     "shell.execute_reply": "2022-02-14T03:27:46.914499Z",
     "shell.execute_reply.started": "2022-02-14T03:27:46.900468Z"
    }
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a56fd291-d67f-4e40-9330-70f209dcb14d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T04:07:31.835210Z",
     "start_time": "2022-02-14T04:07:03.095268Z"
    },
    "execution": {
     "iopub.execute_input": "2022-02-14T03:37:57.117097Z",
     "iopub.status.busy": "2022-02-14T03:37:57.116886Z",
     "iopub.status.idle": "2022-02-14T03:38:36.444204Z",
     "shell.execute_reply": "2022-02-14T03:38:36.443822Z",
     "shell.execute_reply.started": "2022-02-14T03:37:57.117073Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "wv = api.load(\"glove-twitter-50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57d8193",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "Una operación común es recuperar el vocabulario de un modelo. Eso es trivial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "64ea170f-2764-4f8f-b53b-8ab0ba15f809",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T04:37:52.995303Z",
     "start_time": "2022-02-14T04:37:52.990347Z"
    },
    "execution": {
     "iopub.execute_input": "2022-02-14T03:40:44.605165Z",
     "iopub.status.busy": "2022-02-14T03:40:44.604959Z",
     "iopub.status.idle": "2022-02-14T03:40:44.610064Z",
     "shell.execute_reply": "2022-02-14T03:40:44.609649Z",
     "shell.execute_reply.started": "2022-02-14T03:40:44.605143Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "palabra #0/1193514 es <user>\n",
      "palabra #1/1193514 es .\n",
      "palabra #2/1193514 es :\n",
      "palabra #3/1193514 es rt\n",
      "palabra #4/1193514 es ,\n",
      "palabra #5/1193514 es <repeat>\n",
      "palabra #6/1193514 es <hashtag>\n",
      "palabra #7/1193514 es <number>\n",
      "palabra #8/1193514 es <url>\n",
      "palabra #9/1193514 es !\n",
      "palabra #10/1193514 es i\n",
      "palabra #11/1193514 es a\n",
      "palabra #12/1193514 es \"\n",
      "palabra #13/1193514 es the\n",
      "palabra #14/1193514 es ?\n",
      "palabra #15/1193514 es you\n",
      "palabra #16/1193514 es to\n",
      "palabra #17/1193514 es (\n",
      "palabra #18/1193514 es <allcaps>\n",
      "palabra #19/1193514 es <elong>\n"
     ]
    }
   ],
   "source": [
    "for index, word in enumerate(wv.index_to_key):\n",
    "    if index == 20:\n",
    "        break\n",
    "    print(f\"palabra #{index}/{len(wv.index_to_key)} es {word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a419cf24-26e0-4de7-b99d-8d4777565a2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T04:08:08.494078Z",
     "start_time": "2022-02-14T04:08:08.491678Z"
    },
    "execution": {
     "iopub.execute_input": "2022-02-14T03:41:04.661226Z",
     "iopub.status.busy": "2022-02-14T03:41:04.661017Z",
     "iopub.status.idle": "2022-02-14T03:41:04.664034Z",
     "shell.execute_reply": "2022-02-14T03:41:04.663583Z",
     "shell.execute_reply.started": "2022-02-14T03:41:04.661204Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "vec_king = wv['king']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2ea255b-bdde-4efa-b4c8-31d87bfc0793",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T04:08:13.402779Z",
     "start_time": "2022-02-14T04:08:13.393216Z"
    },
    "execution": {
     "iopub.execute_input": "2022-02-14T03:51:40.935303Z",
     "iopub.status.busy": "2022-02-14T03:51:40.935093Z",
     "iopub.status.idle": "2022-02-14T03:51:40.939582Z",
     "shell.execute_reply": "2022-02-14T03:51:40.939201Z",
     "shell.execute_reply.started": "2022-02-14T03:51:40.935280Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.77201  , -0.16548  ,  0.22263  , -0.56608  ,  0.15602  ,\n",
       "       -0.050659 ,  0.076896 ,  0.90058  , -0.22829  , -0.083794 ,\n",
       "       -0.0087308,  0.12425  , -3.6283   , -0.70631  ,  0.3391   ,\n",
       "       -0.26866  ,  0.012886 ,  0.1314   ,  0.13072  ,  0.1594   ,\n",
       "       -0.43884  ,  0.30631  , -0.51841  , -0.86402  ,  0.89706  ,\n",
       "       -0.29222  ,  0.071633 , -0.7285   ,  0.47514  , -0.54581  ,\n",
       "        0.37375  , -0.2815   , -0.82164  , -0.1245   ,  0.06561  ,\n",
       "        0.2686   ,  0.12587  , -0.50189  ,  0.41322  , -0.40509  ,\n",
       "       -0.88866  , -0.71627  , -0.010728 , -0.29513  ,  0.098062 ,\n",
       "        0.47936  ,  0.49517  , -0.30246  ,  0.37465  ,  0.010619 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_king"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420ba0c2",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "Desafortunadamente, el modelo no puede inferir vectores para palabras desconocidas. Esta es una limitación de Word2Vec: si esta limitación le importa, consulte el modelo FastText."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b763e578-8271-4775-a394-0f8db13b7fdd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T03:45:10.205693Z",
     "iopub.status.busy": "2022-02-14T03:45:10.205485Z",
     "iopub.status.idle": "2022-02-14T03:45:10.216440Z",
     "shell.execute_reply": "2022-02-14T03:45:10.216084Z",
     "shell.execute_reply.started": "2022-02-14T03:45:10.205671Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'sanime' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_632518/1175547976.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sanime\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/nlp_basic/lib/python3.9/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key_or_keys)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \"\"\"\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_or_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKEY_TYPES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_or_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkey_or_keys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlp_basic/lib/python3.9/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, key, norm)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \"\"\"\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_norms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlp_basic/lib/python3.9/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_index\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Key '{key}' not present\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Key 'sanime' not present\""
     ]
    }
   ],
   "source": [
    "wv[\"sanime\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6082e94",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "Continuando, Word2Vec admite varias tareas de similitud de palabras listas para usar. Puedes ver cómo la similitud (¿Que similitud será?) disminuye intuitivamente a medida que las palabras se vuelven cada vez menos similares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "68c443a4-bf35-4e6a-ba04-8335ae961387",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T04:13:11.319163Z",
     "start_time": "2022-02-14T04:13:11.314996Z"
    },
    "execution": {
     "iopub.execute_input": "2022-02-14T03:47:38.159949Z",
     "iopub.status.busy": "2022-02-14T03:47:38.159416Z",
     "iopub.status.idle": "2022-02-14T03:47:38.164359Z",
     "shell.execute_reply": "2022-02-14T03:47:38.163950Z",
     "shell.execute_reply.started": "2022-02-14T03:47:38.159925Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'gato'\t'perro'\t0.85\n",
      "'gato'\t'tigre'\t0.65\n",
      "'gato'\t'rana'\t0.38\n",
      "'gato'\t'nube'\t0.28\n",
      "'gato'\t'politica'\t0.15\n"
     ]
    }
   ],
   "source": [
    "pairs = [\n",
    "    ('gato', 'perro'),   \n",
    "    ('gato', 'tigre'),   \n",
    "    ('gato', 'rana'),  \n",
    "    ('gato', 'nube'),   \n",
    "    ('gato', 'politica'),\n",
    "]\n",
    "for w1, w2 in pairs:\n",
    "    print('%r\\t%r\\t%.2f' % (w1, w2, wv.similarity(w1, w2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0264d69-5a56-4226-ab00-ecb7cd212126",
   "metadata": {},
   "source": [
    "Podemos encontrar las palabras más similares de acuerdo a una medida de similaridad ([la similaridad del coseno](https://en.wikipedia.org/wiki/Cosine_similarity)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8335e925-fc4b-4864-9e5c-10f78d8a1751",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T04:15:15.444042Z",
     "start_time": "2022-02-14T04:15:15.411190Z"
    },
    "execution": {
     "iopub.execute_input": "2022-02-14T03:48:26.843790Z",
     "iopub.status.busy": "2022-02-14T03:48:26.843582Z",
     "iopub.status.idle": "2022-02-14T03:48:26.873820Z",
     "shell.execute_reply": "2022-02-14T03:48:26.873516Z",
     "shell.execute_reply.started": "2022-02-14T03:48:26.843768Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>palabra</th>\n",
       "      <th>similaridad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>camion</td>\n",
       "      <td>0.847325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>coche</td>\n",
       "      <td>0.836260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>autobús</td>\n",
       "      <td>0.834142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>helicóptero</td>\n",
       "      <td>0.820165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>avión</td>\n",
       "      <td>0.792418</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       palabra  similaridad\n",
       "0       camion     0.847325\n",
       "1        coche     0.836260\n",
       "2      autobús     0.834142\n",
       "3  helicóptero     0.820165\n",
       "4        avión     0.792418"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(wv.most_similar(positive=['carro', 'camión'], topn=5), columns=[\"palabra\", \"similaridad\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fd5e85-e2e5-4a05-9835-a9db17eff082",
   "metadata": {},
   "source": [
    "¿Cuál de estos no pertenece a la secuencia?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ab8a17b2-d302-45aa-ba9d-6811a5c3507b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T04:15:43.779444Z",
     "start_time": "2022-02-14T04:15:43.776046Z"
    },
    "execution": {
     "iopub.execute_input": "2022-02-14T03:52:47.886432Z",
     "iopub.status.busy": "2022-02-14T03:52:47.886222Z",
     "iopub.status.idle": "2022-02-14T03:52:47.890151Z",
     "shell.execute_reply": "2022-02-14T03:52:47.889740Z",
     "shell.execute_reply.started": "2022-02-14T03:52:47.886410Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carro\n"
     ]
    }
   ],
   "source": [
    "print(wv.doesnt_match(['fuego', 'agua', 'tierra', 'mar', 'aire', 'carro']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fcc523",
   "metadata": {},
   "source": [
    "### Analogias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6d54f7",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "Se ha demostrado que los vectores de palabras *a veces* exhiben la capacidad de resolver analogías.\n",
    "\n",
    "Como ejemplo, para la analogía \"hombre: rey :: mujer: x\" (léase: el hombre es al rey como la mujer es a x), ¿qué es x?\n",
    "\n",
    "En la celda a continuación, se muestra cómo usar vectores de palabras para encontrar x usando la función `most_similar` de la [documentacion de GenSim](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.most_similares). La función encuentra palabras que son más similares a las palabras en la lista \"positiva\" y más diferentes de las palabras en la lista \"negativa\" (mientras omite las palabras de entrada, que a menudo son las más similares). La respuesta a la analogía tendrá la mayor similitud del coseno (mayor valor numérico devuelto)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e38c3d8f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T04:27:39.603586Z",
     "start_time": "2022-02-14T04:27:39.576082Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('reina', 0.823491632938385),\n",
       " ('diana', 0.7247895002365112),\n",
       " ('victoria', 0.724120557308197),\n",
       " ('shakira', 0.7164205312728882),\n",
       " ('chica', 0.7134189009666443),\n",
       " ('luna', 0.6988048553466797),\n",
       " ('colombiana', 0.6971472501754761),\n",
       " ('canción', 0.6866007447242737),\n",
       " ('dulce', 0.6847524046897888),\n",
       " ('karina', 0.6814141869544983)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# esta es la analogia -- hombre : rey :: mujer : x\n",
    "wv.most_similar(positive=['mujer', 'rey'], negative=['hombre'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96fb514",
   "metadata": {},
   "source": [
    "#### Ejercicio 1\n",
    "Cree una función denominada `analogia(a,b,c)` donde determine la analogía *a* : *b* :: *c* : *d*, es decir retorne a *d*, como en el ejemplo anterior. Pruebe la función con 5 analogías que se le ocurra (en inglés o en españól, pero no combinadas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e263c41",
   "metadata": {},
   "source": [
    "#### Ejercicio 2\n",
    "Haga un listado de unas 20 palabras y grafiquelas en dos dimensiones (colocando su texto) junto con unas 200 palabras muestreadas aleatoriamente (sin texto). Use para reducir la dimensión PCA. Repita lo mismo usando UMAP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ddb3c1",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2aac28",
   "metadata": {},
   "source": [
    "GenSim permite acceder a las palabras mediante diferentes formas sobre un objeto `KeyedVectors` (el wv de antes es uno). `.index_to_key`produce una lista con el vocabulario de forma ordenada, mientras `.key_to_index` produce un diccionario de la forma {palabra: index}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e95f75eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T05:09:19.256005Z",
     "start_time": "2022-02-14T05:09:19.253182Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_', 'mi', 'can', '<sadface>', 'من', '♡', '´', 'he', 'con', 'they', 'now', 'go', '،', 'para', 'los', 'know', 'haha', 'good', 'tu', 'back']\n"
     ]
    }
   ],
   "source": [
    "print(wv.index_to_key[100:120])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca19dfb4",
   "metadata": {},
   "source": [
    "## Entrene un nuevo modelo sobre un corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4dc2ed",
   "metadata": {},
   "source": [
    "Trabajaremos de nuevo con el dataset de `progressive-tweet-sentiment.csv`, el cuál es un dataset pequeño que nos facilitará probar Word2vec. Sin embargo, los resultados pueden no ser tan buenos, dado que Word2vec es más potente cuando el corpus es más grande.\n",
    "\n",
    "`progressive-tweet-sentiment`  tiene algunos tweets recopilados y categorizados en 4 clases: 'Legalization of Abortion', 'Hillary Clinton', 'Feminist Movement', 'Atheism'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ab41f401",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T05:11:17.786754Z",
     "start_time": "2022-02-14T05:11:17.773980Z"
    },
    "execution": {
     "iopub.execute_input": "2022-02-14T03:01:40.202685Z",
     "iopub.status.busy": "2022-02-14T03:01:40.202480Z",
     "iopub.status.idle": "2022-02-14T03:01:40.232755Z",
     "shell.execute_reply": "2022-02-14T03:01:40.232412Z",
     "shell.execute_reply.started": "2022-02-14T03:01:40.202661Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/progressive-tweet-sentiment.csv\",encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0f313331",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T05:11:21.586157Z",
     "start_time": "2022-02-14T05:11:21.578731Z"
    },
    "execution": {
     "iopub.execute_input": "2022-02-14T03:01:42.930300Z",
     "iopub.status.busy": "2022-02-14T03:01:42.930096Z",
     "iopub.status.idle": "2022-02-14T03:01:42.946310Z",
     "shell.execute_reply": "2022-02-14T03:01:42.945940Z",
     "shell.execute_reply.started": "2022-02-14T03:01:42.930277Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Legalization of Abortion</td>\n",
       "      <td>Thank you for another day of life Lord. #Chris...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Legalization of Abortion</td>\n",
       "      <td>@rosaryrevival Lovely to use Glorious Mysterie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Legalization of Abortion</td>\n",
       "      <td>@Niall250 good thing is that #DUP have consist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Legalization of Abortion</td>\n",
       "      <td>So, you tell me... is murder okay if the victi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Legalization of Abortion</td>\n",
       "      <td>@HillaryClinton Don't you mean to say (all chi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     target                                              tweet\n",
       "0  Legalization of Abortion  Thank you for another day of life Lord. #Chris...\n",
       "1  Legalization of Abortion  @rosaryrevival Lovely to use Glorious Mysterie...\n",
       "2  Legalization of Abortion  @Niall250 good thing is that #DUP have consist...\n",
       "3  Legalization of Abortion  So, you tell me... is murder okay if the victi...\n",
       "4  Legalization of Abortion  @HillaryClinton Don't you mean to say (all chi..."
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[[\"target\", \"tweet\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b65349",
   "metadata": {},
   "source": [
    "#### Ejercicio 3. \n",
    "Realice procedimientos para preprocesar texto cómo: tokenizar, eliminar stopwords, stemming y lemmatization. Al final, el resultado de dicho procesamiento debe ser un texto (no una lista de palabras). Sugerencia: inicie y finalice con métodos de strings de python como `.join()` o `.split()`. (Esto ya lo han hecho en el taller anterior)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fe4a74",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "Nota: En NLP, a menudo agregamos tokens <START> y <END> para representar el principio y el final de oraciones, párrafos o documentos. En este caso, pueden colocar tokens `<START>` y `<END>` encapsulando cada documento, por ejemplo, \"<START> All that glitters not gold <END>\", y se puede incluir los tokens en el corpus completo. No es necesario hacer esto para el ejercicio, pero sería interesante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dbfea18d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T05:11:29.036609Z",
     "start_time": "2022-02-14T05:11:28.060130Z"
    },
    "execution": {
     "iopub.execute_input": "2022-02-14T03:01:46.847067Z",
     "iopub.status.busy": "2022-02-14T03:01:46.846862Z",
     "iopub.status.idle": "2022-02-14T03:01:47.744949Z",
     "shell.execute_reply": "2022-02-14T03:01:47.744610Z",
     "shell.execute_reply.started": "2022-02-14T03:01:46.847043Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>tweet</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Legalization of Abortion</td>\n",
       "      <td>Thank you for another day of life Lord. #Chris...</td>\n",
       "      <td>thank another day life lord christian catholic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Legalization of Abortion</td>\n",
       "      <td>@rosaryrevival Lovely to use Glorious Mysterie...</td>\n",
       "      <td>rosaryrevival lovely use glorious mystery east...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Legalization of Abortion</td>\n",
       "      <td>@Niall250 good thing is that #DUP have consist...</td>\n",
       "      <td>niall250 good thing dup consistently said murd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Legalization of Abortion</td>\n",
       "      <td>So, you tell me... is murder okay if the victi...</td>\n",
       "      <td>tell murder okay victim mentally disabled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Legalization of Abortion</td>\n",
       "      <td>@HillaryClinton Don't you mean to say (all chi...</td>\n",
       "      <td>hillaryclinton dont mean say child deserve cha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     target  \\\n",
       "0  Legalization of Abortion   \n",
       "1  Legalization of Abortion   \n",
       "2  Legalization of Abortion   \n",
       "3  Legalization of Abortion   \n",
       "4  Legalization of Abortion   \n",
       "\n",
       "                                               tweet  \\\n",
       "0  Thank you for another day of life Lord. #Chris...   \n",
       "1  @rosaryrevival Lovely to use Glorious Mysterie...   \n",
       "2  @Niall250 good thing is that #DUP have consist...   \n",
       "3  So, you tell me... is murder okay if the victi...   \n",
       "4  @HillaryClinton Don't you mean to say (all chi...   \n",
       "\n",
       "                                          text_clean  \n",
       "0  thank another day life lord christian catholic...  \n",
       "1  rosaryrevival lovely use glorious mystery east...  \n",
       "2  niall250 good thing dup consistently said murd...  \n",
       "3          tell murder okay victim mentally disabled  \n",
       "4  hillaryclinton dont mean say child deserve cha...  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Debe quedar algo asi:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8849dc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30a6a9c3",
   "metadata": {},
   "source": [
    "### Entrada al modelo\n",
    "Para entrenar un modelo en GenSim es importante adecuar el texto a una lista de sentencias (y cada sentencia una lista de tokens). Para corpus muy grandes, es mejor crear un iterador (una función que extraiga documento a documento para evitar llenar la memoria). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737a0853",
   "metadata": {},
   "source": [
    "Por ejemplo las tres primeras sentencias del dataset son:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c84c279e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T05:32:34.435602Z",
     "start_time": "2022-02-14T05:32:34.430882Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['thank', 'another', 'day', 'life', 'lord', 'christian', 'catholic', 'teamjesus'], ['rosaryrevival', 'lovely', 'use', 'glorious', 'mystery', 'eastertide', 'mark', 'season', 'prayforpersecutedchurch'], ['niall250', 'good', 'thing', 'dup', 'consistently', 'said', 'murder', 'wrong', 'sf', 'pro', 'murderstill']]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43ba7aed",
   "metadata": {},
   "source": [
    "#### Ejercicio 4\n",
    "Entrene un modelo usando `gensim.models.word2vec.Word2Vec`, partiendo de la siguiente configuración de parámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "id": "1f01079b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T06:56:54.605988Z",
     "start_time": "2022-02-14T06:56:54.603096Z"
    }
   },
   "outputs": [],
   "source": [
    "vector_size = 50 # numero de elementos del vector que representa la palabra\n",
    "min_count = 1 # Ignores all words with total frequency lower than this. \n",
    "workers = 3 # numero de cpu cores\n",
    "sg = b # 0: CBOW, 1: skip-gram\n",
    "window = 5 # Tamano de la ventana de contexto\n",
    "sample = 1e-3 # tasa de submuestreo para terminos frecuentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "id": "fa068fcb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T06:56:55.312146Z",
     "start_time": "2022-02-14T06:56:55.122226Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Word2Vec( #....\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c34b0bb",
   "metadata": {},
   "source": [
    "#### Ejercicio 5\n",
    "Ahora ha obtenido un modelo de Word2vec en el cual tiene una representación embebida de cada palabra. Esta representación la puede extraer para cada palabra usando `model.wv.get_vector(palabra)`. Sin embargo, ¿Qué representación podemos obtener para cada tweet (documento o sentencia) a partir de todas las palabras? \n",
    "\n",
    "Cree una función que extraiga para cada tweet un representación vectorial única (un vector) y añada una nueva columna con esta representación. Sugerencia: una suma (pero será lo mejor?). El resultado es algo similar a la celda siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "id": "ad6c7624",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T06:57:10.570014Z",
     "start_time": "2022-02-14T06:57:10.535283Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>tweet</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>vectors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Legalization of Abortion</td>\n",
       "      <td>Thank you for another day of life Lord. #Chris...</td>\n",
       "      <td>thank another day life lord christian catholic...</td>\n",
       "      <td>[0.0032098123, 0.0031128703, 0.0027382753, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Legalization of Abortion</td>\n",
       "      <td>@rosaryrevival Lovely to use Glorious Mysterie...</td>\n",
       "      <td>rosaryrevival lovely use glorious mystery east...</td>\n",
       "      <td>[0.0029359048, 0.0028640802, 0.0026927434, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Legalization of Abortion</td>\n",
       "      <td>@Niall250 good thing is that #DUP have consist...</td>\n",
       "      <td>niall250 good thing dup consistently said murd...</td>\n",
       "      <td>[0.00285275, 0.0033985889, 0.0021539323, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Legalization of Abortion</td>\n",
       "      <td>So, you tell me... is murder okay if the victi...</td>\n",
       "      <td>tell murder okay victim mentally disabled</td>\n",
       "      <td>[0.0013774268, 0.002832932, -0.0004417422, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Legalization of Abortion</td>\n",
       "      <td>@HillaryClinton Don't you mean to say (all chi...</td>\n",
       "      <td>hillaryclinton dont mean say child deserve cha...</td>\n",
       "      <td>[0.003026036, 0.0016216192, 0.0023738334, 0.00...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     target  \\\n",
       "0  Legalization of Abortion   \n",
       "1  Legalization of Abortion   \n",
       "2  Legalization of Abortion   \n",
       "3  Legalization of Abortion   \n",
       "4  Legalization of Abortion   \n",
       "\n",
       "                                               tweet  \\\n",
       "0  Thank you for another day of life Lord. #Chris...   \n",
       "1  @rosaryrevival Lovely to use Glorious Mysterie...   \n",
       "2  @Niall250 good thing is that #DUP have consist...   \n",
       "3  So, you tell me... is murder okay if the victi...   \n",
       "4  @HillaryClinton Don't you mean to say (all chi...   \n",
       "\n",
       "                                          text_clean  \\\n",
       "0  thank another day life lord christian catholic...   \n",
       "1  rosaryrevival lovely use glorious mystery east...   \n",
       "2  niall250 good thing dup consistently said murd...   \n",
       "3          tell murder okay victim mentally disabled   \n",
       "4  hillaryclinton dont mean say child deserve cha...   \n",
       "\n",
       "                                             vectors  \n",
       "0  [0.0032098123, 0.0031128703, 0.0027382753, 0.0...  \n",
       "1  [0.0029359048, 0.0028640802, 0.0026927434, 0.0...  \n",
       "2  [0.00285275, 0.0033985889, 0.0021539323, 0.003...  \n",
       "3  [0.0013774268, 0.002832932, -0.0004417422, 0.0...  \n",
       "4  [0.003026036, 0.0016216192, 0.0023738334, 0.00...  "
      ]
     },
     "execution_count": 656,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ec3a353",
   "metadata": {},
   "source": [
    "#### Ejercicio 6.\n",
    "Lo que se ha generado antes son unas determinadas variables, para cada tweet, úselas para realizar una clasificación. Para esto divida en un conjunto de entrenamiento y uno de prueba (20%) con una semilla fija (42) y construya un clasificador (recomendado Random Forest). Evalúe el desempeño del clasificador en el conjunto de prueba con el accuracy.  El accuracy debe estar alrededor del 70%. Para lograrlo, tendrá que cambiar la longitud del vector, el ancho de la ventana, si es skip-gram o CBOW, el método de obtención de un sólo documento, etc.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('nlp-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "es",
   "useGoogleTranslate": true
  },
  "vscode": {
   "interpreter": {
    "hash": "cf856c288227c1985b5319abfa2c7eaa1df0528794ab1c4cb755b7b4904e1cb5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
